{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e279eb6a5e7161b7",
   "metadata": {},
   "source": [
    "# HW11 on Data Science course of Sharif University of Technology\n",
    "## Created by: Mohammad Mahdi Hossein Beiky     SI: 400100995\n",
    "## GitHub URL: https://github.com/Mmhb1382/Data-Science-HW11.git\n",
    "\n",
    "### Don't bother yourself to run any of following scripts, it takes decades.\n",
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparing Tasks: Downloading the Fashion-MNIST dataset from kaggle",
   "id": "1d69d761cdd8a52c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T16:25:14.201558Z",
     "start_time": "2025-05-20T16:24:48.069974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    " 1) Point the Kaggle API to the folder where your kaggle.json lives\n",
    " 2) Download and unzip the Fashion‑MNIST dataset from Kaggle into a local data/ folder\n",
    " 3) Load the train and test CSV files into pandas DataFrames and sanity‑check their shapes\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Tell the Kaggle CLI where to find your API token\n",
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = r'C:\\PyCharm\\HW11'  # adjust if your path is different\n",
    "\n",
    "# Step 2: Download and unzip the dataset from Kaggle\n",
    "!kaggle datasets download \\\n",
    "    --unzip \\\n",
    "    --force \\\n",
    "    -d zalando-research/fashionmnist \\\n",
    "    -p ./data\n",
    "\n",
    "# Step 3: Read the CSV files into pandas\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('data/fashion-mnist_train.csv')\n",
    "df_test  = pd.read_csv('data/fashion-mnist_test.csv')\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Training data shape:\", df_train.shape)\n",
    "print(\"Test data shape:    \", df_test.shape)\n"
   ],
   "id": "fb211f2b1b40a290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/zalando-research/fashionmnist\n",
      "License(s): other\n",
      "Downloading fashionmnist.zip to ./data\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/68.8M [00:00<?, ?B/s]\n",
      "100%|##########| 68.8M/68.8M [00:00<00:00, 1.82GB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 785)\n",
      "Test data shape:     (10000, 785)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparing Tasks: Preprocessing",
   "id": "943449466b6a8b71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T17:26:42.285578Z",
     "start_time": "2025-05-20T17:26:41.423487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    " 1) Extract pixel data and labels from our train/test DataFrames\n",
    " 2) Reshape the flat 784‑pixel vectors into (28,28,1) image tensors\n",
    " 3) Normalize pixel values from [0,255] to [0,1]\n",
    " 4) One‑hot encode the integer labels for 10 classes\n",
    " 5) Compute per‑pixel mean/std on the training set and standardize both sets\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Pull apart pixels versus labels\n",
    "X_train_flat = df_train.drop('label', axis=1).values\n",
    "y_train      = df_train['label'].values\n",
    "X_test_flat  = df_test.drop('label', axis=1).values\n",
    "y_test       = df_test['label'].values\n",
    "\n",
    "# Step 2: Turn each 784‑vector into a 28×28×1 image\n",
    "X_train = X_train_flat.reshape(-1, 28, 28, 1).astype('float32')\n",
    "X_test  = X_test_flat.reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "# Step 3: Scale pixels to the [0,1] range\n",
    "X_train /= 255.0\n",
    "X_test  /= 255.0\n",
    "\n",
    "# Step 4: One‑hot encode our labels\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test  = to_categorical(y_test,  num_classes)\n",
    "\n",
    "# Step 5: Standardize based on training‑set statistics\n",
    "mean = np.mean(X_train, axis=0, keepdims=True)\n",
    "std  = np.std(X_train,  axis=0, keepdims=True) + 1e-7  # avoid div0\n",
    "X_train = (X_train - mean) / std\n",
    "X_test  = (X_test  - mean) / std\n"
   ],
   "id": "712d367a5bced344",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 1: Three CNN Variants with 3‑Fold Cross‑Validation",
   "id": "9b9ba312d58258a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T17:38:20.753787Z",
     "start_time": "2025-05-20T17:26:52.305649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    " 1) Define three different ConvNet layouts (options A, B, C)\n",
    " 2) For each: run 3‑fold CV, collect fold accuracies\n",
    " 3) Print per‑fold and average for each option\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# 3 model “options” to try:\n",
    "configs = {\n",
    "    'A': {  # baseline 2-layer small kernels\n",
    "        'layers': [\n",
    "            dict(filters=32, kernel_size=(3,3)),\n",
    "            dict(filters=64, kernel_size=(3,3)),\n",
    "        ]\n",
    "    },\n",
    "    'B': {  # larger kernels\n",
    "        'layers': [\n",
    "            dict(filters=32, kernel_size=(5,5)),\n",
    "            dict(filters=64, kernel_size=(5,5)),\n",
    "        ]\n",
    "    },\n",
    "    'C': {  # extra conv layer + smaller dense head\n",
    "        'layers': [\n",
    "            dict(filters=32, kernel_size=(3,3)),\n",
    "            dict(filters=64, kernel_size=(3,3)),\n",
    "            dict(filters=128, kernel_size=(3,3)),\n",
    "        ],\n",
    "        'dense_units': 64\n",
    "    }\n",
    "}\n",
    "\n",
    "# Full dataset ready from the “Loading & Preprocessing Fashion‑MNIST” section\n",
    "X_full = X_train\n",
    "y_full = y_train\n",
    "\n",
    "for name, cfg in configs.items():\n",
    "    print(f\"\\n=== Option {name} ===\")\n",
    "    fold_accs = []\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_full), start=1):\n",
    "        # split out fold\n",
    "        X_tr, X_val = X_full[tr_idx], X_full[val_idx]\n",
    "        y_tr, y_val = y_full[tr_idx], y_full[val_idx]\n",
    "\n",
    "        # build model\n",
    "        model = Sequential([ Input(shape=X_tr.shape[1:]) ])\n",
    "        # add augmentation if you like, e.g. data_augmentation\n",
    "        for layer_cfg in cfg['layers']:\n",
    "            model.add(Conv2D(**layer_cfg, activation='relu'))\n",
    "            model.add(MaxPooling2D((2,2)))\n",
    "        model.add(Flatten())\n",
    "        # use custom dense size if provided\n",
    "        dense_units = cfg.get('dense_units', 128)\n",
    "        model.add(Dense(dense_units, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # train on raw arrays (no ImageDataGenerator)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        # eval this fold\n",
    "        _, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"Fold {fold} accuracy: {acc:.4f}\")\n",
    "        fold_accs.append(acc)\n",
    "\n",
    "    avg = np.mean(fold_accs)\n",
    "    print(f\"Option {name} avg accuracy: {avg:.4f}\")\n"
   ],
   "id": "eef8a822d69fca10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Option A ===\n",
      "Fold 1 accuracy: 0.9014\n",
      "Fold 2 accuracy: 0.9140\n",
      "Fold 3 accuracy: 0.9104\n",
      "Option A avg accuracy: 0.9086\n",
      "\n",
      "=== Option B ===\n",
      "Fold 1 accuracy: 0.9085\n",
      "Fold 2 accuracy: 0.9083\n",
      "Fold 3 accuracy: 0.9024\n",
      "Option B avg accuracy: 0.9064\n",
      "\n",
      "=== Option C ===\n",
      "Fold 1 accuracy: 0.8850\n",
      "Fold 2 accuracy: 0.8896\n",
      "Fold 3 accuracy: 0.8871\n",
      "Option C avg accuracy: 0.8873\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fold 1 accuracy: 0.9014\n",
    "#### Fold 2 accuracy: 0.9140\n",
    "#### Fold 3 accuracy: 0.9104\n",
    "#### Option A avg accuracy: 0.9086\n",
    "---\n",
    "#### Fold 1 accuracy: 0.9085\n",
    "#### Fold 2 accuracy: 0.9083\n",
    "#### Fold 3 accuracy: 0.9024\n",
    "#### Option B avg accuracy: 0.9064\n",
    "---\n",
    "#### Fold 1 accuracy: 0.8850\n",
    "#### Fold 2 accuracy: 0.8896\n",
    "#### Fold 3 accuracy: 0.8871\n",
    "#### Option C avg accuracy: 0.8873"
   ],
   "id": "6b553442bb92ecb6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 2: Tuning Kernel Size, Stride, Pooling Size & Pooling Stride",
   "id": "b31ce263c68867a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T18:06:33.571218Z",
     "start_time": "2025-05-20T17:45:39.250161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    " 1) Define four variants that each adjust one convolution/pooling parameter\n",
    "     • Variant A: increase kernel size to (5,5)\n",
    "     • Variant B: set convolution stride to 2\n",
    "     • Variant C: increase pooling window to (3,3)\n",
    "     • Variant D: set pooling stride to (1,1)\n",
    " 2) For each variant, run 3‑fold CV and compute average validation accuracy\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Variants to test\n",
    "variants = {\n",
    "    'A_kernel5x5':    {'kernel_size': (5,5), 'conv_stride': 1, 'pool_size': (2,2), 'pool_stride': 2},\n",
    "    'B_convStride2':  {'kernel_size': (3,3), 'conv_stride': 2, 'pool_size': (2,2), 'pool_stride': 2},\n",
    "    'C_pool3x3':      {'kernel_size': (3,3), 'conv_stride': 1, 'pool_size': (3,3), 'pool_stride': 3},\n",
    "    'D_poolStride1':  {'kernel_size': (3,3), 'conv_stride': 1, 'pool_size': (2,2), 'pool_stride': 1},\n",
    "}\n",
    "\n",
    "# Full dataset ready from the “Loading & Preprocessing Fashion‑MNIST” section\n",
    "X_full, y_full = X_train, y_train\n",
    "\n",
    "for name, cfg in variants.items():\n",
    "    print(f\"\\n=== Variant {name} ===\")\n",
    "    fold_accs = []\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold_idx, (tr_idx, val_idx) in enumerate(kf.split(X_full), start=1):\n",
    "        # Split data for this fold\n",
    "        X_tr, X_val = X_full[tr_idx], X_full[val_idx]\n",
    "        y_tr, y_val = y_full[tr_idx], y_full[val_idx]\n",
    "\n",
    "        # Build model with variant-specific settings\n",
    "        model = Sequential([\n",
    "            Input(shape=X_tr.shape[1:]),\n",
    "            Conv2D(32,\n",
    "                   cfg['kernel_size'],\n",
    "                   strides=cfg['conv_stride'],\n",
    "                   activation='relu'),\n",
    "            MaxPooling2D(cfg['pool_size'], strides=cfg['pool_stride']),\n",
    "            Conv2D(64,\n",
    "                   cfg['kernel_size'],\n",
    "                   strides=cfg['conv_stride'],\n",
    "                   activation='relu'),\n",
    "            MaxPooling2D(cfg['pool_size'], strides=cfg['pool_stride']),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Train and evaluate\n",
    "        model.fit(X_tr, y_tr, epochs=10, batch_size=32, verbose=0)\n",
    "        _, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"Fold {fold_idx} accuracy: {acc:.4f}\")\n",
    "        fold_accs.append(acc)\n",
    "\n",
    "    avg_acc = np.mean(fold_accs)\n",
    "    print(f\"Variant {name} average CV accuracy: {avg_acc:.4f}\")\n"
   ],
   "id": "38d7e76b9c0676e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Variant A_kernel5x5 ===\n",
      "Fold 1 accuracy: 0.9025\n",
      "Fold 2 accuracy: 0.9069\n",
      "Fold 3 accuracy: 0.9047\n",
      "Variant A_kernel5x5 average CV accuracy: 0.9047\n",
      "\n",
      "=== Variant B_convStride2 ===\n",
      "Fold 1 accuracy: 0.8647\n",
      "Fold 2 accuracy: 0.8658\n",
      "Fold 3 accuracy: 0.8627\n",
      "Variant B_convStride2 average CV accuracy: 0.8644\n",
      "\n",
      "=== Variant C_pool3x3 ===\n",
      "Fold 1 accuracy: 0.8880\n",
      "Fold 2 accuracy: 0.9005\n",
      "Fold 3 accuracy: 0.8899\n",
      "Variant C_pool3x3 average CV accuracy: 0.8928\n",
      "\n",
      "=== Variant D_poolStride1 ===\n",
      "Fold 1 accuracy: 0.9182\n",
      "Fold 2 accuracy: 0.9229\n",
      "Fold 3 accuracy: 0.9141\n",
      "Variant D_poolStride1 average CV accuracy: 0.9184\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fold 1 accuracy: 0.9025\n",
    "#### Fold 2 accuracy: 0.9069\n",
    "#### Fold 3 accuracy: 0.9047\n",
    "#### Variant A_kernel5x5 average CV accuracy: 0.9047\n",
    "---\n",
    "#### Fold 1 accuracy: 0.8647\n",
    "#### Fold 2 accuracy: 0.8658\n",
    "#### Fold 3 accuracy: 0.8627\n",
    "#### Variant B_convStride2 average CV accuracy: 0.8644\n",
    "---\n",
    "#### Fold 1 accuracy: 0.8880\n",
    "#### Fold 2 accuracy: 0.9005\n",
    "#### Fold 3 accuracy: 0.8899\n",
    "#### Variant C_pool3x3 average CV accuracy: 0.8928\n",
    "---\n",
    "#### Fold 1 accuracy: 0.9182\n",
    "#### Fold 2 accuracy: 0.9229\n",
    "#### Fold 3 accuracy: 0.9141\n",
    "#### Variant D_poolStride1 average CV accuracy: 0.9184"
   ],
   "id": "22809d7485294737"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 3: Data Augmentation with ImageDataGenerator",
   "id": "34731a6db3bbc248"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T18:31:36.782993Z",
     "start_time": "2025-05-20T18:15:31.665598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    " 1) Define three different augmentation pipelines (Options A, B, C)\n",
    " 2) For each pipeline, run 3‑fold CV\n",
    " 3) For each fold, build the same 2‑conv CNN\n",
    " 4) Use ImageDataGenerator.flow() to get augmented batches\n",
    " 5) Train with model.train_on_batch(...) instead of model.fit()\n",
    " 6) Evaluate on raw (X_val, y_val) arrays\n",
    " 7) Report per‑fold and average accuracy for each option\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models    import Sequential\n",
    "from tensorflow.keras.layers    import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Three augmentation configurations to try\n",
    "aug_configs = {\n",
    "    'A': {\n",
    "        'rotation_range': 15,\n",
    "        'width_shift_range': 0.1,\n",
    "        'height_shift_range': 0.1,\n",
    "        'horizontal_flip': True\n",
    "    },\n",
    "    'B': {\n",
    "        'rotation_range': 30,\n",
    "        'zoom_range': 0.2,\n",
    "        'width_shift_range': 0.1,\n",
    "        'height_shift_range': 0.1\n",
    "    },\n",
    "    'C': {\n",
    "        'shear_range': 0.2,\n",
    "        'zoom_range': 0.2,\n",
    "        'horizontal_flip': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Full training arrays from the “Loading & Preprocessing Fashion‑MNIST” section\n",
    "X_full, y_full = X_train, y_train\n",
    "\n",
    "for name, params in aug_configs.items():\n",
    "    print(f\"\\n=== Augmentation Option {name} ===\")\n",
    "    fold_accs = []\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    for fold_idx, (tr_idx, val_idx) in enumerate(kf.split(X_full), start=1):\n",
    "        print(f\"–– Fold {fold_idx} ––\")\n",
    "        X_tr, X_val = X_full[tr_idx], X_full[val_idx]\n",
    "        y_tr, y_val = y_full[tr_idx], y_full[val_idx]\n",
    "\n",
    "        # Build baseline 2‑conv CNN\n",
    "        model = Sequential([\n",
    "            Input(shape=X_tr.shape[1:]),\n",
    "            Conv2D(32, (3,3), activation='relu'),\n",
    "            MaxPooling2D((2,2)),\n",
    "            Conv2D(64, (3,3), activation='relu'),\n",
    "            MaxPooling2D((2,2)),\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        # Create an augmenting generator for this fold\n",
    "        train_datagen = ImageDataGenerator(**params)\n",
    "        train_gen     = train_datagen.flow(X_tr, y_tr, batch_size=32)\n",
    "        steps_per_epoch = len(train_gen)\n",
    "\n",
    "        # Manually train for N epochs over augmented batches\n",
    "        epochs = 10\n",
    "        for epoch in range(epochs):\n",
    "            for step in range(steps_per_epoch):\n",
    "                Xb, yb = next(train_gen)                   # use next() instead of .next()\n",
    "                model.train_on_batch(Xb, yb)\n",
    "\n",
    "        # Evaluate on the raw validation arrays\n",
    "        loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"Fold {fold_idx} accuracy: {acc:.4f}\\n\")\n",
    "        fold_accs.append(acc)\n",
    "\n",
    "    avg_acc = np.mean(fold_accs)\n",
    "    print(f\"Option {name} average CV accuracy: {avg_acc:.4f}\")\n"
   ],
   "id": "d662a41a29510337",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Augmentation Option A ===\n",
      "–– Fold 1 ––\n",
      "Fold 1 accuracy: 0.8745\n",
      "\n",
      "–– Fold 2 ––\n",
      "Fold 2 accuracy: 0.8778\n",
      "\n",
      "–– Fold 3 ––\n",
      "Fold 3 accuracy: 0.8769\n",
      "\n",
      "Option A average CV accuracy: 0.8764\n",
      "\n",
      "=== Augmentation Option B ===\n",
      "–– Fold 1 ––\n",
      "Fold 1 accuracy: 0.8615\n",
      "\n",
      "–– Fold 2 ––\n",
      "Fold 2 accuracy: 0.8719\n",
      "\n",
      "–– Fold 3 ––\n",
      "Fold 3 accuracy: 0.8486\n",
      "\n",
      "Option B average CV accuracy: 0.8606\n",
      "\n",
      "=== Augmentation Option C ===\n",
      "–– Fold 1 ––\n",
      "Fold 1 accuracy: 0.8881\n",
      "\n",
      "–– Fold 2 ––\n",
      "Fold 2 accuracy: 0.8936\n",
      "\n",
      "–– Fold 3 ––\n",
      "Fold 3 accuracy: 0.8860\n",
      "\n",
      "Option C average CV accuracy: 0.8892\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fold 1 accuracy: 0.8745\n",
    "#### Fold 2 accuracy: 0.8778\n",
    "#### Fold 3 accuracy: 0.8769\n",
    "#### Option A average CV accuracy: 0.8764\n",
    "---\n",
    "#### Fold 1 accuracy: 0.8615\n",
    "#### Fold 2 accuracy: 0.8719\n",
    "#### Fold 3 accuracy: 0.8486\n",
    "#### Option B average CV accuracy: 0.8606\n",
    "---\n",
    "#### Fold 1 accuracy: 0.8881\n",
    "#### Fold 2 accuracy: 0.8936\n",
    "#### Fold 3 accuracy: 0.8860\n",
    "#### Option C average CV accuracy: 0.8892"
   ],
   "id": "6680f498eb001817"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task 4: Transfer Learning with VGG19 and ResNet50",
   "id": "1b8a8eb7ed5df232"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T20:36:25.932090Z",
     "start_time": "2025-05-20T18:59:04.885572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    " 1) Expand our (28×28×1) grayscale images to RGB and resize to 32×32\n",
    " 2) Pre‑load VGG19 and ResNet50 “no top” weights from Keras so they cache once\n",
    " 3) For each model, run 3‑fold CV:\n",
    "    • Load base with weights='imagenet', include_top=False\n",
    "    • Freeze the base, attach a new head (GlobalAvgPool → Dense(256) → Dropout → Dense(10))\n",
    "    • Compile, train for 5 epochs (verbose=1), evaluate\n",
    " 4) Print per‑fold and average validation accuracy for each model\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.applications import VGG19, ResNet50\n",
    "from tensorflow.keras.models       import Model\n",
    "from tensorflow.keras.layers       import Input, GlobalAveragePooling2D, Dense, Dropout\n",
    "\n",
    "# Step 1: RGB expand and resize once\n",
    "X_rgb     = np.repeat(X_train, 3, axis=-1)              # (n,28,28,3)\n",
    "X_resized = tf.image.resize(X_rgb, [32,32]).numpy()     # (n,32,32,3)\n",
    "y_full    = y_train\n",
    "\n",
    "# Step 2: Pre‑cache both sets of weights (downloads happen here)\n",
    "_ = VGG19   (weights='imagenet', include_top=False, input_shape=(32,32,3))\n",
    "_ = ResNet50(weights='imagenet', include_top=False, input_shape=(32,32,3))\n",
    "\n",
    "# Step 3: Set up cross‑validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "models = {\n",
    "    'VGG19':    VGG19,\n",
    "    'ResNet50': ResNet50\n",
    "}\n",
    "\n",
    "for name, Constructor in models.items():\n",
    "    print(f\"\\n=== Transfer Learning: {name} ===\")\n",
    "    fold_accs = []\n",
    "\n",
    "    for fold_idx, (tr, val) in enumerate(kf.split(X_resized), start=1):\n",
    "        # split data\n",
    "        X_tr, X_val = X_resized[tr], X_resized[val]\n",
    "        y_tr, y_val = y_full[tr],    y_full[val]\n",
    "\n",
    "        # load and freeze the base\n",
    "        base = Constructor(weights='imagenet',\n",
    "                           include_top=False,\n",
    "                           input_shape=(32,32,3))\n",
    "        base.trainable = False\n",
    "\n",
    "        # attach new classification head\n",
    "        inputs = Input(shape=(32,32,3))\n",
    "        x = base(inputs, training=False)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        outputs = Dense(num_classes, activation='softmax')(x)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # train & validate\n",
    "        print(f\"--- {name} fold {fold_idx} ---\")\n",
    "        model.fit(X_tr, y_tr,\n",
    "                  epochs=5,            # reduced for speed\n",
    "                  batch_size=32,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_val, y_val))\n",
    "        _, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "        print(f\"{name} fold {fold_idx} accuracy: {acc:.4f}\\n\")\n",
    "        fold_accs.append(acc)\n",
    "\n",
    "    avg_acc = np.mean(fold_accs)\n",
    "    print(f\"{name} average 3‑fold CV accuracy: {avg_acc:.4f}\")\n"
   ],
   "id": "9c6ef4dc41c2e235",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001B[1m94765736/94765736\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 0us/step\n",
      "\n",
      "=== Transfer Learning: VGG19 ===\n",
      "--- VGG19 fold 1 ---\n",
      "Epoch 1/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m149s\u001B[0m 118ms/step - accuracy: 0.6964 - loss: 0.8783 - val_accuracy: 0.8132 - val_loss: 0.5096\n",
      "Epoch 2/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m174s\u001B[0m 139ms/step - accuracy: 0.8034 - loss: 0.5386 - val_accuracy: 0.8191 - val_loss: 0.4716\n",
      "Epoch 3/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m174s\u001B[0m 139ms/step - accuracy: 0.8164 - loss: 0.4967 - val_accuracy: 0.8357 - val_loss: 0.4449\n",
      "Epoch 4/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m184s\u001B[0m 147ms/step - accuracy: 0.8275 - loss: 0.4631 - val_accuracy: 0.8341 - val_loss: 0.4391\n",
      "Epoch 5/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m173s\u001B[0m 139ms/step - accuracy: 0.8355 - loss: 0.4463 - val_accuracy: 0.8421 - val_loss: 0.4194\n",
      "VGG19 fold 1 accuracy: 0.8421\n",
      "\n",
      "--- VGG19 fold 2 ---\n",
      "Epoch 1/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m184s\u001B[0m 146ms/step - accuracy: 0.6901 - loss: 0.8744 - val_accuracy: 0.8183 - val_loss: 0.4913\n",
      "Epoch 2/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m176s\u001B[0m 140ms/step - accuracy: 0.8020 - loss: 0.5412 - val_accuracy: 0.8328 - val_loss: 0.4507\n",
      "Epoch 3/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m270s\u001B[0m 216ms/step - accuracy: 0.8177 - loss: 0.4947 - val_accuracy: 0.8395 - val_loss: 0.4342\n",
      "Epoch 4/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m249s\u001B[0m 199ms/step - accuracy: 0.8275 - loss: 0.4662 - val_accuracy: 0.8456 - val_loss: 0.4175\n",
      "Epoch 5/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m179s\u001B[0m 143ms/step - accuracy: 0.8325 - loss: 0.4509 - val_accuracy: 0.8475 - val_loss: 0.4134\n",
      "VGG19 fold 2 accuracy: 0.8475\n",
      "\n",
      "--- VGG19 fold 3 ---\n",
      "Epoch 1/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m177s\u001B[0m 141ms/step - accuracy: 0.6891 - loss: 0.8844 - val_accuracy: 0.8200 - val_loss: 0.4928\n",
      "Epoch 2/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m176s\u001B[0m 141ms/step - accuracy: 0.8013 - loss: 0.5366 - val_accuracy: 0.8311 - val_loss: 0.4604\n",
      "Epoch 3/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m214s\u001B[0m 172ms/step - accuracy: 0.8230 - loss: 0.4881 - val_accuracy: 0.8409 - val_loss: 0.4389\n",
      "Epoch 4/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m248s\u001B[0m 199ms/step - accuracy: 0.8269 - loss: 0.4704 - val_accuracy: 0.8411 - val_loss: 0.4347\n",
      "Epoch 5/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m178s\u001B[0m 142ms/step - accuracy: 0.8365 - loss: 0.4426 - val_accuracy: 0.8446 - val_loss: 0.4275\n",
      "VGG19 fold 3 accuracy: 0.8446\n",
      "\n",
      "VGG19 average 3‑fold CV accuracy: 0.8447\n",
      "\n",
      "=== Transfer Learning: ResNet50 ===\n",
      "--- ResNet50 fold 1 ---\n",
      "Epoch 1/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m124s\u001B[0m 95ms/step - accuracy: 0.6079 - loss: 1.1017 - val_accuracy: 0.7395 - val_loss: 0.6732\n",
      "Epoch 2/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m119s\u001B[0m 95ms/step - accuracy: 0.7299 - loss: 0.7282 - val_accuracy: 0.7519 - val_loss: 0.6318\n",
      "Epoch 3/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m118s\u001B[0m 94ms/step - accuracy: 0.7446 - loss: 0.6851 - val_accuracy: 0.7704 - val_loss: 0.6064\n",
      "Epoch 4/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m119s\u001B[0m 95ms/step - accuracy: 0.7532 - loss: 0.6535 - val_accuracy: 0.7831 - val_loss: 0.5813\n",
      "Epoch 5/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m173s\u001B[0m 138ms/step - accuracy: 0.7565 - loss: 0.6356 - val_accuracy: 0.7790 - val_loss: 0.5697\n",
      "ResNet50 fold 1 accuracy: 0.7790\n",
      "\n",
      "--- ResNet50 fold 2 ---\n",
      "Epoch 1/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m195s\u001B[0m 137ms/step - accuracy: 0.6058 - loss: 1.1182 - val_accuracy: 0.7527 - val_loss: 0.6663\n",
      "Epoch 2/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m120s\u001B[0m 96ms/step - accuracy: 0.7275 - loss: 0.7216 - val_accuracy: 0.7677 - val_loss: 0.6243\n",
      "Epoch 3/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m119s\u001B[0m 95ms/step - accuracy: 0.7436 - loss: 0.6795 - val_accuracy: 0.7767 - val_loss: 0.5982\n",
      "Epoch 4/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m123s\u001B[0m 99ms/step - accuracy: 0.7556 - loss: 0.6551 - val_accuracy: 0.7788 - val_loss: 0.5896\n",
      "Epoch 5/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m271s\u001B[0m 217ms/step - accuracy: 0.7615 - loss: 0.6387 - val_accuracy: 0.7857 - val_loss: 0.5708\n",
      "ResNet50 fold 2 accuracy: 0.7857\n",
      "\n",
      "--- ResNet50 fold 3 ---\n",
      "Epoch 1/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m286s\u001B[0m 210ms/step - accuracy: 0.6064 - loss: 1.1078 - val_accuracy: 0.7420 - val_loss: 0.6729\n",
      "Epoch 2/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m236s\u001B[0m 189ms/step - accuracy: 0.7292 - loss: 0.7212 - val_accuracy: 0.7680 - val_loss: 0.6160\n",
      "Epoch 3/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m235s\u001B[0m 188ms/step - accuracy: 0.7476 - loss: 0.6808 - val_accuracy: 0.7747 - val_loss: 0.6001\n",
      "Epoch 4/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m129s\u001B[0m 103ms/step - accuracy: 0.7526 - loss: 0.6547 - val_accuracy: 0.7868 - val_loss: 0.5732\n",
      "Epoch 5/5\n",
      "\u001B[1m1250/1250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m170s\u001B[0m 136ms/step - accuracy: 0.7647 - loss: 0.6289 - val_accuracy: 0.7858 - val_loss: 0.5685\n",
      "ResNet50 fold 3 accuracy: 0.7858\n",
      "\n",
      "ResNet50 average 3‑fold CV accuracy: 0.7835\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### VGG19 fold 1 accuracy: 0.8421\n",
    "#### VGG19 fold 2 accuracy: 0.8475\n",
    "#### VGG19 fold 3 accuracy: 0.8446\n",
    "\n",
    "#### VGG19 average 3‑fold CV accuracy: 0.8447\n",
    "---\n",
    "#### ResNet50 fold 1 accuracy: 0.7790\n",
    "#### ResNet50 fold 2 accuracy: 0.7857\n",
    "#### ResNet50 fold 3 accuracy: 0.7858\n",
    "\n",
    "#### ResNet50 average 3‑fold CV accuracy: 0.7835"
   ],
   "id": "728f23c21901ab21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 5: Effects of Receptive Field Size in Convolution Layers\n",
    "\n",
    "In convolutional neural networks, the “window size” or kernel size determines each layer’s **receptive field**—the region of the input image that influences a single output activation. Adjusting this size has the following effects:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Increasing the receptive field (larger kernels)\n",
    "\n",
    "- **Broader context per layer**\n",
    "  A 5×5 filter sees a 5×5 patch at once, capturing larger patterns or textures in a single convolution.\n",
    "\n",
    "- **Fewer layers to cover large areas**\n",
    "  Large kernels allow the network to “zoom out” more quickly, which can help when distinguishing global shapes.\n",
    "\n",
    "- **Higher parameter count & compute cost**\n",
    "  A 5×5 kernel has 25 weights per channel versus 9 for a 3×3, increasing memory and FLOPs—and raising overfitting risk if data are limited.\n",
    "\n",
    "- **Smoothed / blurred features**\n",
    "  Aggregating over a larger region can wash out fine details or high‑frequency signals.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Decreasing the receptive field (smaller kernels)\n",
    "\n",
    "- **Fine‑grained detail retention**\n",
    "  A 3×3 filter focuses closely on edges, corners, and small textures.\n",
    "\n",
    "- **Greater non‑linearity per effective field**\n",
    "  Stacking two 3×3 layers yields an effective 5×5 receptive field **plus** two activation functions, enabling richer feature learning with fewer total parameters.\n",
    "\n",
    "- **Deeper networks needed for global context**\n",
    "  To “see” a 7×7 or larger region you must stack more layers, which can slow training and complicate optimization.\n",
    "\n",
    "- **Parameter efficiency**\n",
    "  Modern architectures (VGG, ResNet, EfficientNet) favor 3×3 almost everywhere: they build up large receptive fields via depth rather than expensive wide filters.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Practical trade‑offs (as seen in our variants)\n",
    "\n",
    "- **3×3 kernels (Variant A)**\n",
    "  Lightweight and fast, but requires multiple layers to capture larger patterns.\n",
    "\n",
    "- **5×5 kernels (Variant B)**\n",
    "  Captures broader context immediately, but at ~3× the parameters of a 3×3 layer and with potential loss of small details.\n",
    "\n",
    "- **Three 3×3 layers (Variant C)**\n",
    "  Achieves an effective 7×7 field **plus** extra non‑linear transforms, often outperforming a single 5×5 layer while remaining more efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### Why it matters\n",
    "\n",
    "- **Depth + small kernels** is generally best for expressive power, regularization, and training stability.\n",
    "- **Large kernels** can be useful in early layers or very low‑resolution inputs, but should be applied sparingly.\n",
    "\n",
    "State‑of‑the‑art CNNs almost universally use 3×3 filters, reserving larger windows only where global context is essential.\n"
   ],
   "id": "d0670ddb67ace9fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
